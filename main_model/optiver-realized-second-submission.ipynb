{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### This is the submission that got me 7th place. It's not very readable and there are a lot of things to be improved so sorry about that, but I've been told to share it as soon as I could. This is the version using future information. See [link to practical version as soon as I finish it] for the simpler, leak-free version.\n### The main ideas can be found here: https://www.kaggle.com/c/optiver-realized-volatility-prediction/discussion/276506#1534495","metadata":{}},{"cell_type":"markdown","source":"#### Import libraries, input paths etc.","metadata":{}},{"cell_type":"code","source":"from IPython.core.display import display, HTML\nimport glob\n\npath_submissions = '/'\n# data directory\ndata_dir = '../input/optiver-realized-volatility-prediction/'\nall_train_book = glob.glob(f'/kaggle/input/optiver-realized-volatility-prediction/book_train.parquet/**/*.parquet')\nall_test_book = glob.glob(f'/kaggle/input/optiver-realized-volatility-prediction/book_test.parquet/**/*.parquet')\n\nimport pandas as pd\nimport numpy as np\nimport os\nimport gc\n\nfrom math import sqrt\nfrom joblib import Parallel, delayed\nfrom sklearn import preprocessing, model_selection\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import minmax_scale\nfrom sklearn.neighbors import NearestNeighbors\nimport lightgbm as lgb\nfrom sklearn.cluster import KMeans\nfrom tensorflow.keras.backend import sigmoid\nfrom itertools import chain\nfrom itertools import permutations\nfrom math import floor\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport numpy.matlib\nfrom tensorflow.keras.utils import get_custom_objects\nfrom tensorflow.keras.layers import Activation\n\nimport random\nfrom numpy.random import seed\nseed(42)\nimport tensorflow as tf\ntf.random.set_seed(42)\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\nfrom scipy.linalg import lstsq\nimport seaborn as sns","metadata":{"_uuid":"d6754be8-e93b-4c15-94fb-06422055c799","_cell_guid":"c537ab37-e863-46c3-82d2-54caaa2287af","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-22T16:35:12.333487Z","iopub.execute_input":"2022-01-22T16:35:12.334536Z","iopub.status.idle":"2022-01-22T16:35:21.499415Z","shell.execute_reply.started":"2022-01-22T16:35:12.334398Z","shell.execute_reply":"2022-01-22T16:35:21.498356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Function to retrieve an array of usable time_ids.","metadata":{}},{"cell_type":"code","source":"def get_time_ids(is_train = True):\n    files = all_train_book if is_train else all_test_book\n    return np.sort(pd.concat([pd.read_parquet(file, columns = ['time_id']) for file in files], ignore_index = True)['time_id'].unique())","metadata":{"_uuid":"01b9b6cc-e5b1-46cd-b460-b510054136c9","_cell_guid":"b3486d9e-d36d-4346-a21f-def8d920ead2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-21T13:24:28.02813Z","iopub.execute_input":"2022-01-21T13:24:28.028578Z","iopub.status.idle":"2022-01-21T13:24:28.033755Z","shell.execute_reply.started":"2022-01-21T13:24:28.028535Z","shell.execute_reply":"2022-01-21T13:24:28.032629Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### List to split each time_id.","metadata":{}},{"cell_type":"code","source":"time_splits = [0, 100, 200, 300, 400, 500, 600]","metadata":{"_uuid":"58a75dc6-b713-4c73-a581-c77311618ed1","_cell_guid":"4bcc8ef4-929b-494e-a3cc-82f7fe790356","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-21T13:24:28.035439Z","iopub.execute_input":"2022-01-21T13:24:28.035903Z","iopub.status.idle":"2022-01-21T13:24:28.04599Z","shell.execute_reply.started":"2022-01-21T13:24:28.035853Z","shell.execute_reply":"2022-01-21T13:24:28.044829Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Account for the test dataset potentially not having enough time_ids.","metadata":{}},{"cell_type":"code","source":"test_time_id_len = len(get_time_ids(False))\n\nis_dummy = test_time_id_len == 1\nprox = test_time_id_len if test_time_id_len in range(2,6) else 6\n\nif test_time_id_len == 1:\n    unord_group_size = [20, 50, 100]\nelif test_time_id_len in range(2,7):\n    unord_group_size = []\nelif test_time_id_len in range(7,24):\n    unord_group_size = [test_time_id_len]\nelif test_time_id_len in range(24,48):\n    unord_group_size = [round(test_time_id_len/2), test_time_id_len]\nelif test_time_id_len in range(48, 100):\n    unord_group_size = [round(test_time_id_len/4), round(test_time_id_len/2), test_time_id_len]\nelse:\n    unord_group_size = [20, 50, 100]","metadata":{"_uuid":"d06686dc-77b2-4a32-8646-6222c512e2fb","_cell_guid":"9c8c6a3a-bc85-4df4-9b91-43f899a756c4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-21T13:24:28.048038Z","iopub.execute_input":"2022-01-21T13:24:28.048741Z","iopub.status.idle":"2022-01-21T13:24:28.151815Z","shell.execute_reply.started":"2022-01-21T13:24:28.048701Z","shell.execute_reply":"2022-01-21T13:24:28.150849Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Define various functions for feature engineering.","metadata":{}},{"cell_type":"code","source":"def log_return(series):\n    return np.log(series).diff()\n\n# Function to calculate first WAP\ndef calc_wap1(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n    return wap\n\n# Function to calculate second WAP\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n    return wap\n\n# Calculate the realized volatility\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\n# Function to count unique elements of a series\ndef count_unique(series):\n    return len(np.unique(series))\n\n# RQQ replaced rolling window with convolve for performance\ndef realized_quadpower_quarticity(series):\n    convolved = np.exp(np.convolve(np.log(series.to_numpy()), [1, 1, 1, 1], mode='valid'))\n    return (np.sum(convolved) * (convolved.shape[0]+3) * (np.pi**2))/4\n\ndef tendency(price, vol):    \n    df_diff = np.diff(price.to_numpy())\n    val = (df_diff/price.to_numpy()[1:])*100\n    power = np.sum(val*vol.to_numpy()[1:])\n    return(power)\n\ndef iqr(series): \n    return np.percentile(series.to_numpy(),75) - np.percentile(series.to_numpy(),25)\n\ndef abs_diff(series): \n    return np.median(np.abs(series.to_numpy() - np.mean(series.to_numpy())))\n\ndef energy(series): \n    return np.mean(series.to_numpy()**2)\n\ndef f_max(series): \n    return np.sum(series.to_numpy() > np.mean(series.to_numpy()))\n\ndef f_min(series): \n    return np.sum(series.to_numpy() < np.mean(series.to_numpy()))\n\ndef df_max(series): \n    return np.sum(np.diff(series.to_numpy()) > 0)\n\ndef df_min(series): \n    return np.sum(np.diff(series.to_numpy()) < 0)","metadata":{"_uuid":"f478d836-8ddc-4f0f-8961-208e234442b2","_cell_guid":"28b47f67-a08f-423e-aec6-8faa402ebdc9","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-21T13:24:28.154405Z","iopub.execute_input":"2022-01-21T13:24:28.154741Z","iopub.status.idle":"2022-01-21T13:24:28.171087Z","shell.execute_reply.started":"2022-01-21T13:24:28.154701Z","shell.execute_reply":"2022-01-21T13:24:28.170444Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Fix broken offsets (in public test dataset)","metadata":{}},{"cell_type":"code","source":"def list_names(dic, prefix = '', splits = []):\n    return list(chain(*[[f'{prefix}{key}_{func.__name__}{t}' for func in lis] for key, lis in dic.items() for t in ['']+splits]))\n\ndef fix_offsets(book_df, trade_df):\n    offsets = book_df.groupby(['time_id']).agg({'seconds_in_bucket':'min'})\n    offsets.columns = ['offset']\n    book_df = book_df.join(offsets, on='time_id')\n    trade_df = trade_df.join(offsets, on='time_id')\n    book_df.seconds_in_bucket = book_df.seconds_in_bucket - book_df.offset\n    trade_df.seconds_in_bucket = trade_df.seconds_in_bucket - trade_df.offset\n    book_df.drop(columns=['offset'], inplace=True)\n    trade_df.drop(columns=['offset'], inplace=True)\n    return book_df, trade_df\n\ndef fix_offsets_book(data_df):\n    offsets = data_df.groupby(['time_id']).agg({'seconds_in_bucket':'min'})\n    offsets.columns = ['offset']\n    data_df = data_df.join(offsets, on='time_id')\n    data_df.seconds_in_bucket = data_df.seconds_in_bucket - data_df.offset\n    return data_df\n\ndef ffill(data_df):\n    data_df=data_df.set_index(['time_id', 'seconds_in_bucket'])\n    data_df = data_df.reindex(pd.MultiIndex.from_product([data_df.index.levels[0], np.arange(0,600)], names = ['time_id', 'seconds_in_bucket']), method='ffill')\n    return data_df.reset_index()","metadata":{"_uuid":"a129bb96-bd26-4572-aa2f-94020ca8c057","_cell_guid":"a2d0edc2-362d-4bca-b2d0-6c851b72f71d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-21T13:24:28.173049Z","iopub.execute_input":"2022-01-21T13:24:28.173558Z","iopub.status.idle":"2022-01-21T13:24:28.188223Z","shell.execute_reply.started":"2022-01-21T13:24:28.173514Z","shell.execute_reply":"2022-01-21T13:24:28.18728Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### De-normalize stock prices using tick sizes, inspired by [this](https://www.kaggle.com/c/optiver-realized-volatility-prediction/discussion/256725#1449558) code snippet by @stassi. Generate arrays for the beginning, average and end prices of each time_id.","metadata":{}},{"cell_type":"code","source":"def calc_price(df):\n    book = df.to_numpy()a\n    diff = np.diff(book.transpose().flatten())\n    mindiff = np.nanmin(abs(diff[diff != 0]))\n    return book[:60, [0, 2]].mean() * 0.01 / mindiff, book[:, [0, 2]].mean() * 0.01 / mindiff, book[-60:, [0, 2]].mean() * 0.01 / mindiff\n\ndef calc_prices(r, is_start = True):\n    df = ffill(fix_offsets_book(pd.read_parquet(r.book_path)))\n    df = df[['time_id', 'ask_price1', 'ask_price2', 'bid_price1', 'bid_price2']]\n    df = df.set_index('time_id')\n    df = df.groupby(level='time_id').apply(calc_price).to_frame('price').reset_index()\n    df['stock_id'] = r.stock_id\n    return df\n\ndef calc_price_2(df):\n    book = df.to_numpy()\n    diff = np.diff(book.transpose().flatten())\n    mindiff = np.nanmin(abs(diff[diff != 0]))\n    return book[0, [0, 2]].mean() * 0.01 / mindiff, book[-1, [0, 2]].mean() * 0.01 / mindiff","metadata":{"_uuid":"863842e5-6883-403b-8fbe-b4fb9feb264c","_cell_guid":"817e7bc0-42d9-478d-a46a-493754354015","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-21T13:24:28.18934Z","iopub.execute_input":"2022-01-21T13:24:28.190793Z","iopub.status.idle":"2022-01-21T13:24:28.203602Z","shell.execute_reply.started":"2022-01-21T13:24:28.190748Z","shell.execute_reply":"2022-01-21T13:24:28.202911Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_book_files(is_train = True):\n    df_files = pd.DataFrame(\n        {'book_path': all_train_book if is_train else all_test_book}) \\\n        .eval('stock_id = book_path.str.extract(\"stock_id=(\\d+)\").astype(\"int\")', engine='python')\n    df_prices = pd.concat(Parallel(n_jobs=4, verbose=51)(delayed(calc_prices)(r) for _, r in df_files.iterrows()))\n    return df_prices.pivot(index = 'time_id', columns = 'stock_id', values = 'price')\n\ndef get_price_dfs(is_train = True):\n    df_prices = get_book_files(is_train)\n    df_prices = df_prices.dropna(axis = 1)\n    arr = np.array([[list(x) for x in time_id] for time_id in df_prices.to_numpy()])\n    arr = np.array([(arr[:, i, :] - arr[:, i, :].min())/(arr[:, i, :].max() - arr[:, i, :].min()) for i in range(arr.shape[1])])\n    np_start = arr[..., 0].transpose()\n    np_avg = arr[..., 1].transpose()\n    np_end = arr[..., 2].transpose()\n    df_start = pd.DataFrame(np_start, index = df_prices.index, columns = df_prices.columns)\n    df_avg = pd.DataFrame(np_avg, index = df_prices.index, columns = df_prices.columns)\n    df_end = pd.DataFrame(np_end, index = df_prices.index, columns = df_prices.columns)\n    return df_start, df_avg, df_end","metadata":{"_uuid":"8cd76b74-4c31-4ea1-bae8-c7c5cb4d45bb","_cell_guid":"23fc8c39-3ea1-4465-bb40-612e1cb5cf44","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-21T13:24:28.20681Z","iopub.execute_input":"2022-01-21T13:24:28.207037Z","iopub.status.idle":"2022-01-21T13:24:28.220159Z","shell.execute_reply.started":"2022-01-21T13:24:28.207012Z","shell.execute_reply":"2022-01-21T13:24:28.219329Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_prices(stock_id, list_time_ids, is_train = True):\n    df_path = f'{data_dir}book_{\"train\" if is_train else \"test\"}.parquet/stock_id={str(stock_id)}/'\n    df = pd.read_parquet(df_path, columns = ['time_id', 'ask_price1', 'ask_price2', 'bid_price1', 'bid_price2'])\n    df = df.set_index('time_id')\n    df = df.groupby(level='time_id').apply(calc_price_2).to_frame('price')\n    df = df.reindex(list_time_ids)\n    df = df.applymap(lambda x: (-1,-1) if x is np.nan else x)\n    arrr = df['price']\n    np_start = np.array([val[0] for val in arrr])\n    np_end = np.array([val[1] for val in arrr])\n    return np_start, np_end","metadata":{"_uuid":"f6aae0a4-e7d8-4ea9-a759-433fc174a9c9","_cell_guid":"e192ef0a-a331-48b8-929f-932d7b34f620","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-21T13:24:28.221458Z","iopub.execute_input":"2022-01-21T13:24:28.221928Z","iopub.status.idle":"2022-01-21T13:24:28.235221Z","shell.execute_reply.started":"2022-01-21T13:24:28.22189Z","shell.execute_reply":"2022-01-21T13:24:28.234178Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### All of the cluster stuff was added there as a botched way to make this work with datasets of different sizes. There is obviously a much better way to do it (like using something like $\\sqrt{\\sum{}{}(\\log{(\\frac{u_i}{v_i})})^2}$ instead of Eucledean distances between $u_i$ and $v_i$ for NN distances) but unfortunately I've only thought of that after the submission deadline. This also accounted for the potential presence of filler data in the dataset.\n","metadata":{}},{"cell_type":"code","source":"def get_neighbors(df_avg, is_train = True):\n    np_avg = df_avg.to_numpy()\n    if len(df_avg) == 1:\n        n_nei = 1\n    elif unord_group_size:\n        n_nei = unord_group_size[-1]\n    else:\n        n_nei = prox\n    if is_train:\n        n_clusters = 12\n        cluster_list, nbrs_list, distances, indices = [], [], [], []\n        kmeans = KMeans(n_clusters = n_clusters, random_state = 0).fit(df_avg.to_numpy())\n        for i in range(n_clusters):\n            cluster_ind = np.arange(np_avg.shape[0])[kmeans.labels_ == i]\n            cluster_list.append(cluster_ind)\n            nbrs_list.append(NearestNeighbors(n_neighbors=n_nei+30).fit(np_avg[cluster_ind]))\n        for i, vector in enumerate(np_avg):\n            curr_group = kmeans.labels_[i]\n            dist, indi = nbrs_list[curr_group].kneighbors([vector])\n            dist, indi = dist[0], indi[0]\n            to_delete = set(random.sample(range(1,len(indi)), 30))\n            dist = np.array([d for i,d in enumerate(dist) if i not in to_delete])\n            indi = np.array([d for i,d in enumerate(indi) if i not in to_delete])\n            distances.append(dist)\n            indices.append(cluster_list[curr_group][indi])\n        return np.array(distances), np.array(indices)\n    else: \n        avg_nbrs = NearestNeighbors(n_neighbors=n_nei).fit(np_avg)\n        return avg_nbrs.kneighbors(np_avg)","metadata":{"_uuid":"f693060f-bd8c-476f-927b-2ed9e2b53d1e","_cell_guid":"2435faac-b1a2-42c7-abea-5cf5cad31d64","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-21T13:24:28.23773Z","iopub.execute_input":"2022-01-21T13:24:28.238359Z","iopub.status.idle":"2022-01-21T13:24:28.25141Z","shell.execute_reply.started":"2022-01-21T13:24:28.238324Z","shell.execute_reply":"2022-01-21T13:24:28.250742Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Brute-force the shortest Hamilton path between the nearest time_ids in an attempt to re-order them: \n\n#### Take arrays of start and end prices of each time_id, and re-order them to minimise the Euclidean distance.","metadata":{}},{"cell_type":"code","source":"def get_ordered_prox(df_start, df_end, indices):\n    np_start = df_start.to_numpy()\n    np_end = df_end.to_numpy()\n    def vec_distance(vec1, vec2):\n        return np.sqrt(np.sum((vec1 - vec2)**2))\n    def get_total_distance(seq):\n        return sum([vec_distance(np_start[i], np_end[j]) for i,j in zip(seq[1:], seq[:-1])])\n    indices = np.array([list(list(permutations(a))[np.argmin(list(map(get_total_distance, permutations(a))))]) for a in indices])\n    distances = np.array([list(chain(*[[vec_distance(np_start[j], np_end[j]), vec_distance(np_start[i], np_end[j])] for i, j in zip(seq[1:], seq[:-1])] \\\n                                + [[vec_distance(np_start[seq[-1]], np_end[seq[-1]])]])) for seq in indices])\n    return distances, indices","metadata":{"_uuid":"79ba7497-56f0-493b-9081-df3e589c5688","_cell_guid":"e52dd8e7-9803-4f46-a791-5f58150713c7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-21T13:24:28.252627Z","iopub.execute_input":"2022-01-21T13:24:28.253559Z","iopub.status.idle":"2022-01-21T13:24:28.267327Z","shell.execute_reply.started":"2022-01-21T13:24:28.253525Z","shell.execute_reply":"2022-01-21T13:24:28.266166Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Function to get the mean, trend and error of an array using least squares. This is used to process features generated in time windows to reduce dimensionality and improve interpretability of feature importance.","metadata":{}},{"cell_type":"code","source":"def get_dir_stats(arr):\n    coefs, err, _, _ = lstsq(np.array([np.ones(len(arr)), np.arange(len(arr))]).transpose(), arr)\n    return np.array([coefs[0] + len(arr)*coefs[1], coefs[1], err])","metadata":{"_uuid":"5154743f-fd13-4329-af69-99fb73a5c094","_cell_guid":"f5ccd4e6-0d03-4780-94ca-eeaed1245b8f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-21T13:24:28.26858Z","iopub.execute_input":"2022-01-21T13:24:28.268898Z","iopub.status.idle":"2022-01-21T13:24:28.282226Z","shell.execute_reply.started":"2022-01-21T13:24:28.268873Z","shell.execute_reply":"2022-01-21T13:24:28.281239Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Get stats from the nearest time_ids.","metadata":{}},{"cell_type":"code","source":"def get_prox_stats(df, ord_distances, ord_indices, start_prices, end_prices):\n    # INDEX IN ORD\n    final_df = pd.DataFrame()\n    ind = np.array([np.where(elem == i)[0][0] for i, elem in enumerate(ord_indices)])\n    # USED COLS / FUNCS\n    next_group_list = ['log_return1_realized_volatility_intercept', \n                       'total_volume_sum', \n                       'trade_order_count_sum_intercept',\n                       'trade_order_count_sum_slope',\n                       'trade_size_sum_intercept',\n                       'trade_size_sum_slope',\n                       'trade_size_sum_error',\n                       'log_return1_realized_volatility_slope', \n                       'log_return1_realized_volatility_error']\n    prox_group_list = ['log_return1_realized_volatility_intercept', \n                       'log_return1_realized_volatility_slope', \n                       'log_return1_realized_volatility_error', \n                       'trade_order_count_sum_intercept']\n    func_list = [np.mean, np.std, np.min, np.max]\n    # RETURN 0 IF ONLY 1 COL\n    if len(ord_indices[0]) == 1:\n        name_list = [f'prox_{col}_{func.__name__}' for col in prox_group_list for func in func_list] + \\\n        [f'next_{col}' for col in next_group_list] + \\\n        [f'dist_{func.__name__}' for func in func_list] + \\\n        ['dist_window_rv', 'dist_wlr_mean', 'dist_wlr_std', 'dist_wlr_amin', 'dist_wlr_amax', 'dist_to_next', 'next_price_diff', 'ind']\n        return pd.DataFrame(data = np.zeros((len(ord_indices[0]), len(name_list))), columns = name_list)\n    # PROX AND NEXT\n    for col in next_group_list:\n        final_df[f'next_{col}'] = np.array([df[col][smth[curr+1]] if curr < prox-1 else -1 for curr, smth in zip(ind, ord_indices)])\n    for col in prox_group_list:\n        for func in func_list:\n            final_df[f'prox_{col}_{func.__name__}'] = func(np.array([df.iloc[ord_indices[index]][col] for index in df.index]), axis = 1)\n    # DISTANCES\n    window_log_return = [pd.Series(chain(*[[start_prices[t_id], end_prices[t_id]] for t_id in ord_indices[index]])) for index in df.index]\n    window_log_return = [log_return(p_list)[1:] for p_list in window_log_return]\n    final_df['dist_window_rv'] = np.array([realized_volatility(wlr) for wlr in window_log_return])\n    final_df['dist_wlr_std'] = np.array([wlr.std() if wlr.isnull().values.any() == False else -1 for wlr in window_log_return])\n    for func in [np.mean, np.min, np.max]:\n        final_df[f'dist_wlr_{func.__name__}'] = func(window_log_return, axis = 1)\n    final_df['dist_to_next'] = np.array([distance[curr * 2 + 1] if curr < prox-1 else -1 for distance, curr in zip(ord_distances, ind)])\n    for func in func_list:\n        final_df[f'dist_{func.__name__}'] = func(ord_distances, axis = 1)\n    final_df['ind'] = ind\n    price_diff_next = []\n    for end_price, curr, smth in zip(end_prices, ind, ord_indices):\n        if curr >= prox-1 or start_prices[smth[curr+1]] == -1 or end_prices[smth[curr]] == -1:\n            price_diff_next.append(-1)\n        else:\n            price_diff_next.append(abs(np.log(end_price / start_prices[smth[curr+1]])))\n    final_df['next_price_diff'] = np.array(price_diff_next)\n    return final_df","metadata":{"_uuid":"63a5e13e-eddf-4b4a-810c-54f82b831481","_cell_guid":"be3d372b-3dc0-4ee0-b79d-e8e05a20d85c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-21T13:24:28.283822Z","iopub.execute_input":"2022-01-21T13:24:28.284171Z","iopub.status.idle":"2022-01-21T13:24:28.305453Z","shell.execute_reply.started":"2022-01-21T13:24:28.284134Z","shell.execute_reply":"2022-01-21T13:24:28.304562Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Get the n closest time_ids.","metadata":{}},{"cell_type":"code","source":"def get_unord_prox(df, distances, indices, prices, count, dum):\n    cols = ['log_return1_realized_volatility_intercept', 'total_volume_sum', 'trade_log_return_realized_volatility_intercept']\n    func_list = [np.mean, np.std, np.min, np.max]\n    func_list_dist = [np.mean, np.std, np.min, np.max] if count == unord_group_size[0] else [np.mean, np.std, np.max]\n    final_df = pd.DataFrame()\n    if count == unord_group_size[0]:\n        final_df[f'prox_{count}_price_min_max'] = [0] if dum else np.array([prices[ind].min()/prices[ind].max() for ind in indices])\n        final_df[f'prox_{count}_price_mean_max'] = [0] if dum else np.array([prices[ind].mean()/prices[ind].max() for ind in indices])\n    for func in func_list:\n        for col in cols:\n            final_df[f'prox_{count}_{col}_{func.__name__}'] = [0] if dum else func(np.array([df.iloc[indices[index]][col] for index in df.index]), axis = 1)\n    for func in func_list_dist:\n        final_df[f'dist_{count}_{func.__name__}'] = [0] if dum else func(distances[...,1:], axis = 1)\n    return final_df","metadata":{"_uuid":"f4e1f00c-304a-425a-8d37-500a019d41d6","_cell_guid":"07248b3a-070d-41d5-9803-948573e78734","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-21T13:24:28.307056Z","iopub.execute_input":"2022-01-21T13:24:28.308377Z","iopub.status.idle":"2022-01-21T13:24:28.323454Z","shell.execute_reply.started":"2022-01-21T13:24:28.30834Z","shell.execute_reply":"2022-01-21T13:24:28.322547Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Process book data (through combinations of the above functions)","metadata":{}},{"cell_type":"code","source":"# Function to preprocess book data (for each stock id)\ndef book_preprocessor(df, stock_id, time_ids):\n    # Calculate Wap\n    df['wap1'] = calc_wap1(df)\n    df['wap2'] = calc_wap2(df)\n    # Calculate log returns\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    # Calculate wap balance\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    # Calculate spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'wap1': [np.mean, realized_quadpower_quarticity, count_unique],\n        'wap2': [np.mean, realized_quadpower_quarticity, count_unique],\n        'log_return1': [np.sum, np.min, np.max],\n        'log_return2': [np.sum],\n        'wap_balance': [np.mean, np.max],\n        'price_spread':[np.mean, np.max],\n        'price_spread2':[np.mean, np.max],\n        'bid_spread':[np.mean, np.max],\n        'ask_spread':[np.mean, np.max],\n        'total_volume':[np.sum],\n        'volume_imbalance':[np.mean],\n        \"bid_ask_spread\":[np.mean, np.max],\n    }\n    \n    create_feature_dict_time = {\n        'log_return1': [realized_volatility],\n        'log_return2': [realized_volatility],\n    }\n    \n    df_feature = df.groupby(['time_id']).agg(create_feature_dict).reset_index()\n    df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n    df_feature = df_feature.set_index('time_id_').reindex(pd.Series(time_ids))\n    df_features = []\n    for start, stop in zip(time_splits[:-1], time_splits[1:]):\n        df_tmpp = df[(df['seconds_in_bucket'] >= start) & (df['seconds_in_bucket'] < stop)].groupby(['time_id']).agg(create_feature_dict_time).reset_index()\n        df_tmpp.columns = ['_'.join(col) for col in df_tmpp.columns]\n        df_tmpp = df_tmpp.set_index('time_id_').reindex(pd.Series(time_ids))\n        df_tmpp = df_tmpp.fillna(0)\n        df_features.append(df_tmpp[list_names(create_feature_dict_time)].to_numpy().transpose())\n    df_features = np.stack(df_features, axis = 2)\n    df_features = np.apply_along_axis(get_dir_stats, 2, df_features)\n    for name, vals in zip(list_names(create_feature_dict_time), df_features):\n        for attr, col in zip(['intercept', 'slope', 'error'], vals.transpose()):\n            df_feature[f'{name}_{attr}'] = col\n    \n    # Create row_id so we can merge\n    df_feature.reset_index(inplace = True)\n    df_feature['row_id'] = df_feature['index'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['index'], axis = 1, inplace = True)\n    return df_feature","metadata":{"_uuid":"8997af63-ffed-4520-87d1-b0059cfde80c","_cell_guid":"c06f089a-c578-484b-aefb-8020b877f088","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-21T13:24:28.325165Z","iopub.execute_input":"2022-01-21T13:24:28.325735Z","iopub.status.idle":"2022-01-21T13:24:28.35129Z","shell.execute_reply.started":"2022-01-21T13:24:28.325693Z","shell.execute_reply":"2022-01-21T13:24:28.350558Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Process trade data.","metadata":{}},{"cell_type":"code","source":"# Function to preprocess trade data (for each stock id)\ndef trade_preprocessor(df, stock_id, time_ids):\n    #df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    df['amount']=df['price']*df['size']\n    # Dict for aggregations\n    create_feature_dict = {\n        'amount':[np.min],\n        'price':[abs_diff, energy, iqr]\n    }\n    \n    create_feature_dict_time = {\n        'log_return':[realized_volatility],\n        'order_count':[np.sum],\n        'size':[np.sum]\n    }\n    \n    df_feature = df.groupby(['time_id']).agg(create_feature_dict).reset_index()\n    df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n    df_feature = df_feature.set_index('time_id_').reindex(pd.Series(time_ids))\n    df_features = []\n    for start, stop in zip(time_splits[:-1], time_splits[1:]):\n        df_tmpp = df[(df['seconds_in_bucket'] >= start) & (df['seconds_in_bucket'] < stop)].groupby(['time_id']).agg(create_feature_dict_time).reset_index()\n        df_tmpp.columns = ['_'.join(col) for col in df_tmpp.columns]\n        df_tmpp = df_tmpp.set_index('time_id_').reindex(pd.Series(time_ids))\n        df_tmpp = df_tmpp.fillna(0)\n        df_features.append(df_tmpp[list_names(create_feature_dict_time)].to_numpy().transpose())\n    df_features = np.stack(df_features, axis = 2)\n    df_features = np.apply_along_axis(get_dir_stats, 2, df_features)\n    for name, vals in zip(list_names(create_feature_dict_time), df_features):\n        for attr, col in zip(['intercept', 'slope', 'error'], vals.transpose()):\n            df_feature[f'{name}_{attr}'] = col\n    df_feature.reset_index(inplace = True)\n\n    # Get the stats for different windows\n    df_feature = df_feature.add_prefix('trade_')\n    df_feature['row_id'] = df_feature['trade_index'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_index'], axis = 1, inplace = True)\n    return df_feature","metadata":{"_uuid":"8a327f11-a7ba-446d-97f3-37711decafbc","_cell_guid":"84208a79-6234-4595-9334-808174668c05","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-21T13:24:28.35268Z","iopub.execute_input":"2022-01-21T13:24:28.353288Z","iopub.status.idle":"2022-01-21T13:24:28.370106Z","shell.execute_reply.started":"2022-01-21T13:24:28.353243Z","shell.execute_reply":"2022-01-21T13:24:28.369294Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trivial_nan_once = {\n    'amount':[np.min],\n    'price':[iqr]\n}","metadata":{"_uuid":"0978195f-32ef-46ea-bc4d-db77ee8e09dc","_cell_guid":"badeb598-76ab-40e7-83d3-d85c0c76ac12","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-21T13:24:28.371428Z","iopub.execute_input":"2022-01-21T13:24:28.37176Z","iopub.status.idle":"2022-01-21T13:24:28.383304Z","shell.execute_reply.started":"2022-01-21T13:24:28.37172Z","shell.execute_reply":"2022-01-21T13:24:28.382569Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Get time_id aggregated stats.","metadata":{}},{"cell_type":"code","source":"def get_time_stats(df):\n    vol_cols = [f'log_return1_realized_volatility_{t}' for t in ['intercept', 'slope', 'error']]\n    vol_cols += ['trade_order_count_sum_intercept', 'next_trade_order_count_sum_intercept', 'trade_order_count_sum_slope', 'next_trade_order_count_sum_slope']\n    vol_cols += [f'next_log_return1_realized_volatility_{t}' for t in ['intercept', 'slope', 'error']]\n    vol_cols += [f'prox_log_return1_realized_volatility_intercept_{func}' for func in ['mean', 'std', 'amin', 'amax']]\n    vol_cols += [f'prox_{num}_log_return1_realized_volatility_intercept_{func}' for num in unord_group_size for func in ['mean', 'std']]\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    df.drop(['time_id__time'], axis = 1, inplace = True)\n    return df","metadata":{"_uuid":"567023f3-d3bb-4c40-b731-76ed176c1245","_cell_guid":"230da815-32b7-4ad8-8bee-39b1da39bf8b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-21T13:24:28.384192Z","iopub.execute_input":"2022-01-21T13:24:28.386434Z","iopub.status.idle":"2022-01-21T13:24:28.395641Z","shell.execute_reply.started":"2022-01-21T13:24:28.386332Z","shell.execute_reply":"2022-01-21T13:24:28.394702Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Combine all above function.","metadata":{}},{"cell_type":"code","source":"# Funtion to make preprocessing function in parallel (for each stock id)\ndef preprocessor(list_stock_ids, is_train = True):\n    # Parrallel for loop\n    df_start, df_avg, df_end = get_price_dfs(is_train)\n    unord_dist, unord_ind = get_neighbors(df_avg, is_train)\n    distances, indices = get_ordered_prox(df_start, df_end, unord_ind[...,:prox])\n    list_time_ids = get_time_ids(is_train)\n    def for_joblib(stock_id):\n        np_start, np_end = get_prices(stock_id, list_time_ids, is_train)\n        # Train\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n        # Test\n        else:\n            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n            \n        book_df, trade_df = fix_offsets(pd.read_parquet(file_path_book), pd.read_parquet(file_path_trade))\n        # Preprocess book and trade data and merge them\n        df_tmp = pd.merge(book_preprocessor(book_df, stock_id, list_time_ids), trade_preprocessor(trade_df, stock_id, list_time_ids), on = 'row_id', how = 'left')\n        colls = list_names(trivial_nan_once, 'trade_')\n        for col in colls:\n            df_tmp[col] = df_tmp[col].fillna(0)\n        # Return the merge dataframe\n        df_tmp = df_tmp.join(get_prox_stats(df_tmp, distances, indices, np_start, np_end))\n        for i in unord_group_size:\n            df_tmp = df_tmp.join(get_unord_prox(df_tmp, unord_dist[...,:i], unord_ind[...,:i], np_start, i, is_dummy and not is_train))\n        return df_tmp\n    \n    # Use parallel api to call paralle for loop\n    df = Parallel(n_jobs = -1, verbose = 51)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    # Concatenate all the dataframes that return from Parallel\n    df = pd.concat(df, ignore_index = True)\n    return df","metadata":{"_uuid":"a50d8151-d27d-48ba-a04b-322e83b7900e","_cell_guid":"bae4af47-a27a-48ba-bd12-47dabfee3a3b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-21T13:24:28.397029Z","iopub.execute_input":"2022-01-21T13:24:28.39737Z","iopub.status.idle":"2022-01-21T13:24:28.410982Z","shell.execute_reply.started":"2022-01-21T13:24:28.397341Z","shell.execute_reply":"2022-01-21T13:24:28.410423Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Drop unnecessary features.","metadata":{}},{"cell_type":"code","source":"def generate_df(is_train):\n    df = pd.read_csv(f'{data_dir}{\"train\" if is_train else \"test\"}.csv')\n    df['row_id'] = df['stock_id'].astype(str) + '-' + df['time_id'].astype(str)\n    print(f'Our {\"training\" if is_train else \"testing\"} set has {df.shape[0]} rows')\n    stock_ids = df['stock_id'].unique()\n    # Preprocess them using Parallel and our single stock id functions\n    pre_ = preprocessor(stock_ids, is_train = is_train)\n    df = df.merge(pre_, on = ['row_id'], how = 'left')\n    df = get_time_stats(df)\n    df.drop(columns = [f'prox_trade_order_count_sum_intercept_{func}' for func in ['amax', 'std']], inplace = True)\n    df.drop(columns = ['prox_log_return1_realized_volatility_slope_amin'])\n    df.drop(columns = [f'trade_size_sum_{att}' for att in ['error', 'slope']], inplace = True)\n    if len(unord_group_size) == 3:\n        for grp in unord_group_size:\n            if grp != unord_group_size[1]:\n                df.drop(columns = [f'prox_{grp}_total_volume_sum_std'], inplace = True)\n            if grp == unord_group_size[0]:\n                df.drop(columns = [f'prox_{grp}_trade_log_return_realized_volatility_intercept_std'], inplace = True)\n                df.drop(columns = [f'prox_{grp}_log_return1_realized_volatility_intercept_amax'], inplace = True)\n            if grp == unord_group_size[2]:\n                df.drop(columns = [f'prox_{grp}_total_volume_sum_amax'], inplace = True)\n    return df","metadata":{"_uuid":"d211c8d2-9805-4aae-9009-bf200fe361b3","_cell_guid":"fdc13973-72d1-4b85-8a13-643e4a41cbd2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-21T13:24:28.411902Z","iopub.execute_input":"2022-01-21T13:24:28.412693Z","iopub.status.idle":"2022-01-21T13:24:28.425522Z","shell.execute_reply.started":"2022-01-21T13:24:28.41266Z","shell.execute_reply":"2022-01-21T13:24:28.424534Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Generate train and test DFs.","metadata":{}},{"cell_type":"code","source":"test = generate_df(False)","metadata":{"_uuid":"4d20fd98-f8cb-4b94-a709-2bd67c7b2ca1","_cell_guid":"c8b9184b-6222-4658-aeef-c0145b0a4e8f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-21T13:24:28.426701Z","iopub.execute_input":"2022-01-21T13:24:28.427069Z","iopub.status.idle":"2022-01-21T13:24:30.069899Z","shell.execute_reply.started":"2022-01-21T13:24:28.427018Z","shell.execute_reply":"2022-01-21T13:24:30.068544Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = generate_df(True)","metadata":{"_uuid":"37af3b17-1504-4881-8d13-e35b9f2eaa31","_cell_guid":"2bc77417-55f9-403d-894f-4949495eb177","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col_names = [col for col in list(train.columns)\n            if col not in {\"stock_id\", \"time_id\", \"target\", \"row_id\"}]\ncflen = len(col_names)","metadata":{"_uuid":"eaf24060-dc4e-480a-bada-d6904ebcb672","_cell_guid":"c6e44f3e-2980-4fd9-ae3b-826edf448b41","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Print correlation heatmap.","metadata":{}},{"cell_type":"code","source":"corr = train[col_names].corr()\nmask = np.triu(corr)\nsns.heatmap(corr, xticklabels = corr.columns, yticklabels = corr.columns, annot = True, ax = plt.subplots(figsize = (200,100))[1], mask = mask)","metadata":{"_uuid":"58c964f4-0322-4abe-a235-c7619f0c89dd","_cell_guid":"c14c515c-9b6b-46d4-b85d-c2e1159ceefd","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Define LGBM model.","metadata":{}},{"cell_type":"code","source":"# Function to calculate the root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n\n# Function to early stop with root mean squared percentage error\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False","metadata":{"_uuid":"fc6c8f4a-dfe6-4f1a-b8bb-6c9cccb64549","_cell_guid":"9a36da34-e5ca-4bd1-87a7-d4b7ebb15dae","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed0 = 2021\nparams0 = {\n    'objective': 'rmse',\n    'boosting_type': 'gbdt',\n    'max_depth': -1,\n    'max_bin':100,\n    'min_data_in_leaf':500,\n    'learning_rate': 0.05,\n    'subsample': 0.72,\n    'subsample_freq': 4,\n    'feature_fraction': 0.5,\n    'lambda_l1': 0.5,\n    'lambda_l2': 1.0,\n    'categorical_column':[0],\n    'seed':seed0,\n    'feature_fraction_seed': seed0,\n    'bagging_seed': seed0,\n    'drop_seed': seed0,\n    'data_random_seed': seed0,\n    'n_jobs':-1,\n    'verbose': -1}","metadata":{"_uuid":"84ad794c-6a11-45b2-be3b-0d0e4abcf35a","_cell_guid":"6762fb63-704a-443a-b8b8-0f087bcd00db","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_and_evaluate_lgb(train, test, params):\n    # Hyperparammeters (just basic)\n    \n    features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\", \"stock_id\"}]\n    y = train['target']\n    # Create out of folds array\n    oof_predictions = np.zeros(train.shape[0])\n    # Create test array to store predictions\n    test_predictions = np.zeros(test.shape[0])\n    # Create a KFold object\n    kfold = KFold(n_splits = 5, random_state = 2021, shuffle = True)\n    # Iterate through each fold\n    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train)):\n        print(f'Training fold {fold + 1}')\n        x_train, x_val = train.iloc[trn_ind], train.iloc[val_ind]\n        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n        # Root mean squared percentage error weights\n        train_weights = 1 / np.square(y_train)\n        val_weights = 1 / np.square(y_val)\n        train_dataset = lgb.Dataset(x_train[features], y_train, weight = train_weights)\n        val_dataset = lgb.Dataset(x_val[features], y_val, weight = val_weights)\n        model = lgb.train(params = params,\n                          num_boost_round=1000,\n                          train_set = train_dataset, \n                          valid_sets = [train_dataset, val_dataset], \n                          verbose_eval = 250,\n                          early_stopping_rounds=50,\n                          feval = feval_rmspe)\n        # Add predictions to the out of folds array\n        oof_predictions[val_ind] = model.predict(x_val[features])\n        # Predict the test set\n        test_predictions += model.predict(test[features]) / 5\n    rmspe_score = rmspe(y, oof_predictions)\n    print(f'Our out of folds RMSPE is {rmspe_score}')\n    lgb.plot_importance(model, figsize = (30, 160))\n    # Return test predictions\n    return test_predictions\n# Traing and evaluate\npredictions_lgb = train_and_evaluate_lgb(train, test, params0)","metadata":{"_uuid":"17b0da58-bfb1-44d2-a3be-cea6214b02fb","_cell_guid":"b3093e4a-1c38-4f2c-81f6-6a4b6fc0fe60","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Define FFNN model.","metadata":{}},{"cell_type":"code","source":"def root_mean_squared_per_error(y_true, y_pred):\n         return K.sqrt(K.mean(K.square((y_true - y_pred) / y_true)))\n    \nes = tf.keras.callbacks.EarlyStopping(\n    monitor = 'val_loss', patience = 20, verbose=0,\n    mode = 'min', restore_best_weights = True)\n\nplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor = 'val_loss', factor = 0.2, patience = 7, verbose = 0,\n    mode = 'min')","metadata":{"_uuid":"95c4c2a9-fc87-4e69-9fd5-49cac291cb23","_cell_guid":"dc391b0a-61e7-49a9-91d0-e394733dddb6","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# kfold based on the knn++ algorithm\n\nout_train = pd.read_csv(f'{data_dir}train.csv')\nout_train = out_train.pivot(index = 'time_id', columns = 'stock_id', values = 'target')\n\nout_train = out_train.fillna(out_train.mean())\nout_train.head()\n\n# code to add the just the read data after first execution\n# data separation based on knn ++\nn_folds = 5 # number of folds\nindex = []\ntotDist = []\nvalues = []\n# generates a matriz with the values of \nmat = out_train.to_numpy()\nscaler = MinMaxScaler(feature_range = (-1, 1))\nmat = scaler.fit_transform(mat)\nnind = int(mat.shape[0]/n_folds) # number of individuals\n\n# adds index in the last column\nmat = np.c_[mat,np.arange(mat.shape[0])]\nlineNumber = np.random.choice(np.array(mat.shape[0]), size=n_folds, replace=False)\nlineNumber = np.sort(lineNumber)[::-1]\n\nfor n in range(n_folds):\n    totDist.append(np.zeros(mat.shape[0]-n_folds))\n\n# saves index\nfor n in range(n_folds):  \n    values.append([lineNumber[n]])    \n\ns = []\nfor n in range(n_folds):\n    s.append(mat[lineNumber[n],:])\n    mat = np.delete(mat, obj = lineNumber[n], axis=0)\n\nfor n in range(nind - 1):    \n    luck = np.random.uniform(0,1,n_folds) \n    for cycle in range(n_folds):\n         # saves the values of index           \n        s[cycle] = np.matlib.repmat(s[cycle], mat.shape[0], 1)\n        sumDist = np.sum( (mat[:,:-1] - s[cycle][:,:-1])**2 , axis=1)   \n        totDist[cycle] += sumDist        \n                \n        # probabilities\n\n        f = totDist[cycle]/np.sum(totDist[cycle]) # normalizing the totdist\n        j = 0\n        kn = 0\n        for val in f:\n            j += val        \n            if (j > luck[cycle]): # the column was selected\n                break\n            kn += 1\n        lineNumber[cycle] = kn\n        \n        # delete line of the value added    \n        for n_iter in range(n_folds):\n            totDist[n_iter] = np.delete(totDist[n_iter],obj = lineNumber[cycle], axis = 0)\n            j = 0\n        \n        s[cycle] = mat[lineNumber[cycle],:]\n        values[cycle].append(int(mat[lineNumber[cycle],-1]))\n        mat = np.delete(mat, obj=lineNumber[cycle], axis = 0)\n\nfor n_mod in range(n_folds):\n    values[n_mod] = out_train.index[values[n_mod]]","metadata":{"_uuid":"29b0f844-949b-4af7-9158-7d2570c38bf3","_cell_guid":"fb3d010d-38e2-4063-88aa-48df4cdb18fb","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.drop(columns = ['row_id'], inplace = True)\ntrain.replace([np.inf, -np.inf], np.nan, inplace = True)\ntest.replace([np.inf, -np.inf], np.nan, inplace = True)\nfor col in col_names:\n    qt = QuantileTransformer(random_state = 21, n_quantiles = 2000, output_distribution = 'normal')\n    train[col] = qt.fit_transform(train[[col]])\n    test[col] = qt.transform(test[[col]])","metadata":{"_uuid":"1b04764b-db21-498a-93ad-5eff1217411d","_cell_guid":"1ac034e5-de3f-4bf6-90f7-13b5dbd93b4b","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#https://bignerdranch.com/blog/implementing-swish-activation-function-in-keras/\ndef swish(x, beta = 1):\n    return (x * sigmoid(beta * x))\n\nget_custom_objects().update({'swish': Activation(swish)})\n\ndef base_model():\n    inputs = keras.Input(shape=(cflen,), name = 'num_data')\n    x = keras.layers.Dense(128, activation = 'swish')(inputs)\n    x = keras.layers.Dense(64, activation = 'swish')(x)\n    x = keras.layers.Dense(32, activation = 'swish')(x)\n    outputs = keras.layers.Dense(1, activation = 'linear', name = 'prediction')(x)\n    model = keras.Model(\n        inputs = inputs,\n        outputs = outputs,\n    )\n    return model","metadata":{"_uuid":"22909d71-252f-4a72-a21e-68865e3fcf07","_cell_guid":"9d0d51f0-099d-461a-b185-dca3625c7f5d","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores_folds = {}\nmodel_name = 'NN'\npred_name = f'pred_{model_name}'\n\nkf = model_selection.KFold(n_splits = n_folds, shuffle=True, random_state=2020)\nscores_folds[model_name] = []\ncounter = 1\n\nfeatures_to_consider = col_names\n\ntrain[features_to_consider] = train[features_to_consider].fillna(train[features_to_consider].mean())\ntest[features_to_consider] = test[features_to_consider].fillna(train[features_to_consider].mean())\n\ntrain[pred_name] = 0\ntest['target'] = 0\ntest_predictions_nn = np.zeros(test.shape[0])\n\nfor n_count in range(n_folds):\n    print(f'CV {counter}/{n_folds}')\n    \n    indexes = np.arange(n_folds).astype(int)    \n    indexes = np.delete(indexes, obj = n_count, axis = 0) \n    indexes = np.r_[values[indexes[0]], values[indexes[1]], values[indexes[2]], values[indexes[3]]]\n    \n    X_train = train.loc[train['time_id'].isin(indexes), features_to_consider]\n    y_train = train.loc[train['time_id'].isin(indexes), 'target']\n    X_test = train.loc[train['time_id'].isin(values[n_count]), features_to_consider]\n    y_test = train.loc[train['time_id'].isin(values[n_count]), 'target']\n\n    # Model    \n    model = base_model()\n    \n    model.compile(\n        keras.optimizers.Adam(learning_rate=0.006),\n        loss=root_mean_squared_per_error\n    )\n    \n    num_data = X_train[features_to_consider]\n    scaler = MinMaxScaler(feature_range = (-1, 1))         \n    num_data = scaler.fit_transform(num_data.to_numpy())    \n    target = y_train\n    \n    num_data_test = X_test[features_to_consider]\n    num_data_test = scaler.transform(num_data_test.to_numpy())\n\n    model.fit(num_data, \n              target,               \n              batch_size = 2048,\n              epochs = 1000,\n              validation_data = (num_data_test, y_test),\n              callbacks = [es, plateau],\n              validation_batch_size = len(y_test),\n              shuffle = True,\n              verbose = 1)\n\n    preds = model.predict(num_data_test).reshape(1,-1)[0]\n    \n    score = round(rmspe(y_true = y_test, y_pred = preds),5)\n    print(f'Fold {counter} {model_name}: {score}')\n    scores_folds[model_name].append(score)\n    \n    tt = scaler.transform(test[features_to_consider].to_numpy())\n    test_predictions_nn += model.predict(tt).reshape(1,-1)[0].clip(0,1e10)/n_folds\n       \n    counter += 1","metadata":{"_uuid":"79f93aac-331c-4df6-a0b3-619082457bcb","_cell_guid":"03ab732f-6e1c-4cab-bdee-a21cbf215860","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Output predictions.","metadata":{}},{"cell_type":"code","source":"test[\"row_id\"] = test[\"stock_id\"].astype(str) + \"-\" + test[\"time_id\"].astype(str) \ntest['target'] = 0.5 * test_predictions_nn + 0.5 * predictions_lgb\n\nscore = round(rmspe(y_true = train['target'].values, y_pred = train[pred_name].values),5)\nprint(f'RMSPE {model_name}: {score} - Folds: {scores_folds[model_name]}')\n\ndisplay(test[['row_id', 'target']].head(3))\ntest[['row_id', 'target']].to_csv('submission.csv',index = False)","metadata":{"_uuid":"54aeccd8-5f9d-4952-bf58-0588721955da","_cell_guid":"c59be0e3-94af-419e-817a-c15c79ed3227","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]}]}